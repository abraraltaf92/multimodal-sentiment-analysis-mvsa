{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffbf1937-b8d7-4460-b6ff-18a149262906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID         text,image       text,image.1       text,image.2\n",
      "0  2499  positive,positive    neutral,neutral  positive,positive\n",
      "1  2500   neutral,positive  positive,positive    neutral,neutral\n",
      "2  2501  negative,negative    neutral,neutral  positive,positive\n",
      "3  2502  positive,positive  positive,positive  positive,positive\n",
      "4  2503   positive,neutral  positive,negative  positive,positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the label file\n",
    "df = pd.read_csv(\"MVSA/labelResultAll.txt\", sep=\"\\t\")\n",
    "\n",
    "# Show first 5 rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6e80d38-98f8-4316-b39e-f96735b8f029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 19600\n",
      "\n",
      "Column names:\n",
      "Index(['\\ID', 'text,image', 'text,image.1', 'text,image.2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Total samples:\", len(df))\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae9e34b1-a339-40b6-94aa-ccff43a9c50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotator columns: ['text,image', 'text,image.1', 'text,image.2']\n",
      "     ID ann1_text ann1_image ann2_text ann2_image ann3_text ann3_image\n",
      "0  2499  positive   positive   neutral    neutral  positive   positive\n",
      "1  2500   neutral   positive  positive   positive   neutral    neutral\n",
      "2  2501  negative   negative   neutral    neutral  positive   positive\n",
      "3  2502  positive   positive  positive   positive  positive   positive\n",
      "4  2503  positive    neutral  positive   negative  positive   positive\n",
      "\n",
      "Columns in labels_df: ['ID', 'ann1_text', 'ann1_image', 'ann2_text', 'ann2_image', 'ann3_text', 'ann3_image']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) Rename the first column to a clean 'ID'\n",
    "id_old_name = df.columns[0]\n",
    "df = df.rename(columns={id_old_name: \"ID\"})\n",
    "\n",
    "# 2) The next three columns are the raw annotator columns\n",
    "annotator_raw_cols = df.columns[1:4]   # assumes structure: ID, text,image, text,image.1, text,image.2\n",
    "print(\"Annotator columns:\", annotator_raw_cols.tolist())\n",
    "\n",
    "# 3) Create a new clean table for labels\n",
    "labels_df = pd.DataFrame()\n",
    "labels_df[\"ID\"] = df[\"ID\"].astype(str).str.strip()\n",
    "\n",
    "# 4) For each annotator column, split into ann{i}_text and ann{i}_image\n",
    "for i, col in enumerate(annotator_raw_cols, start=1):\n",
    "    s = df[col].astype(str).str.strip()\n",
    "    split_cols = s.str.split(\",\", n=1, expand=True)\n",
    "\n",
    "    # If there is only one part (no comma), make the second part NaN\n",
    "    if split_cols.shape[1] == 1:\n",
    "        split_cols[1] = np.nan\n",
    "\n",
    "    labels_df[f\"ann{i}_text\"]  = split_cols[0].str.strip()\n",
    "    labels_df[f\"ann{i}_image\"] = split_cols[1].str.strip()\n",
    "\n",
    "# 5) Check the result: this is what you'll use from now on\n",
    "print(labels_df.head())\n",
    "print(\"\\nColumns in labels_df:\", labels_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e150a87-474e-446d-acb3-b4c1b89393c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID ann1_text ann2_text ann3_text final_label\n",
      "0  2499  positive   neutral  positive    positive\n",
      "1  2500   neutral  positive   neutral     neutral\n",
      "2  2501  negative   neutral  positive     neutral\n",
      "3  2502  positive  positive  positive    positive\n",
      "4  2503  positive  positive  positive    positive\n",
      "\n",
      "Final label distribution:\n",
      "final_label\n",
      "positive    9367\n",
      "neutral     9108\n",
      "negative    1125\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) List of all 6 label columns\n",
    "label_cols = [\n",
    "    \"ann1_text\", \"ann1_image\",\n",
    "    \"ann2_text\", \"ann2_image\",\n",
    "    \"ann3_text\", \"ann3_image\"\n",
    "]\n",
    "\n",
    "def majority_vote_row(row):\n",
    "    # Collect all non-null labels for this row\n",
    "    labels = []\n",
    "    for col in label_cols:\n",
    "        val = row[col]\n",
    "        if pd.isna(val):\n",
    "            continue\n",
    "        val = str(val).strip().lower()\n",
    "        if val in [\"positive\", \"neutral\", \"negative\"]:\n",
    "            labels.append(val)\n",
    "    \n",
    "    # If no valid labels for some reason\n",
    "    if not labels:\n",
    "        return np.nan\n",
    "\n",
    "    # Count frequency\n",
    "    counts = pd.Series(labels).value_counts()\n",
    "    max_count = counts.max()\n",
    "    top_labels = counts[counts == max_count].index.tolist()\n",
    "\n",
    "    # If only one clear winner, use it\n",
    "    if len(top_labels) == 1:\n",
    "        return top_labels[0]\n",
    "\n",
    "    # Tie-break rule: prefer neutral if present, else just pick the first\n",
    "    if \"neutral\" in top_labels:\n",
    "        return \"neutral\"\n",
    "    return top_labels[0]\n",
    "\n",
    "# 2) Apply majority vote to each row to get final_label\n",
    "labels_df[\"final_label\"] = labels_df.apply(majority_vote_row, axis=1)\n",
    "\n",
    "# 3) Inspect result\n",
    "print(labels_df[[\"ID\", \"ann1_text\", \"ann2_text\", \"ann3_text\", \"final_label\"]].head())\n",
    "\n",
    "print(\"\\nFinal label distribution:\")\n",
    "print(labels_df[\"final_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98a35837-63bc-4d10-acff-8e0f73d81bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved clean labels to: /Users/sahilanjum/Desktop/MVSA/labels_clean.csv\n",
      "     ID final_label\n",
      "0  2499    positive\n",
      "1  2500     neutral\n",
      "2  2501     neutral\n",
      "3  2502    positive\n",
      "4  2503    positive\n",
      "\n",
      "Checking first 5 IDs for matching .txt and .jpg files:\n",
      "\n",
      "ID: 2499 | txt exists: True | jpg exists: True\n",
      "  txt: /Users/sahilanjum/Desktop/MVSA/data/2499.txt\n",
      "  img: /Users/sahilanjum/Desktop/MVSA/data/2499.jpg\n",
      "ID: 2500 | txt exists: True | jpg exists: True\n",
      "  txt: /Users/sahilanjum/Desktop/MVSA/data/2500.txt\n",
      "  img: /Users/sahilanjum/Desktop/MVSA/data/2500.jpg\n",
      "ID: 2501 | txt exists: True | jpg exists: True\n",
      "  txt: /Users/sahilanjum/Desktop/MVSA/data/2501.txt\n",
      "  img: /Users/sahilanjum/Desktop/MVSA/data/2501.jpg\n",
      "ID: 2502 | txt exists: True | jpg exists: True\n",
      "  txt: /Users/sahilanjum/Desktop/MVSA/data/2502.txt\n",
      "  img: /Users/sahilanjum/Desktop/MVSA/data/2502.jpg\n",
      "ID: 2503 | txt exists: True | jpg exists: True\n",
      "  txt: /Users/sahilanjum/Desktop/MVSA/data/2503.txt\n",
      "  img: /Users/sahilanjum/Desktop/MVSA/data/2503.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# === 1) Make a clean labels table with only what you need for training ===\n",
    "clean_labels = labels_df[[\"ID\", \"final_label\"]].copy()\n",
    "\n",
    "# Path to your MVSA folder (adjust if needed)\n",
    "BASE_DIR = \"/Users/sahilanjum/Desktop/MVSA\"\n",
    "LABELS_PATH = os.path.join(BASE_DIR, \"labels_clean.csv\")\n",
    "\n",
    "# Save the clean labels to a CSV\n",
    "clean_labels.to_csv(LABELS_PATH, index=False)\n",
    "print(f\"Saved clean labels to: {LABELS_PATH}\")\n",
    "print(clean_labels.head())\n",
    "\n",
    "\n",
    "# === 2) (Optional but very useful) check that text + image files exist ===\n",
    "\n",
    "# Adjust this if your data folder has a different name\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "\n",
    "def id_to_paths(id_str):\n",
    "    txt_path = os.path.join(DATA_DIR, f\"{id_str}.txt\")\n",
    "    img_path = os.path.join(DATA_DIR, f\"{id_str}.jpg\")\n",
    "    return txt_path, img_path\n",
    "\n",
    "print(\"\\nChecking first 5 IDs for matching .txt and .jpg files:\\n\")\n",
    "for _, row in clean_labels.head(5).iterrows():\n",
    "    ID = str(row[\"ID\"])\n",
    "    txt_path, img_path = id_to_paths(ID)\n",
    "    print(\n",
    "        f\"ID: {ID} | \"\n",
    "        f\"txt exists: {os.path.exists(txt_path)} | \"\n",
    "        f\"jpg exists: {os.path.exists(img_path)}\"\n",
    "    )\n",
    "    # If you want to see the paths, uncomment this:\n",
    "    print(\"  txt:\", txt_path)\n",
    "    print(\"  img:\", img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a793414e-9853-4e8a-a65f-ed9c49d53615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19600 entries, 0 to 19599\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ID           19600 non-null  object\n",
      " 1   final_label  19600 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 306.4+ KB\n"
     ]
    }
   ],
   "source": [
    "clean_labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "408711ee-f9c5-4132-8d70-e404630759ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>final_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2499</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2500</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2501</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2502</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2503</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID final_label\n",
       "0  2499    positive\n",
       "1  2500     neutral\n",
       "2  2501     neutral\n",
       "3  2502    positive\n",
       "4  2503    positive"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7fa77254-5768-4b84-9fe3-56bef5c7c0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 19600\n",
      "Keys in one sample: dict_keys(['input_ids', 'attention_mask', 'label', 'id'])\n",
      "input_ids shape: torch.Size([64])\n",
      "label: tensor(2) → positive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Load the clean labels file\n",
    "labels_path = os.path.join(BASE_DIR, \"labels_clean.csv\")\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "\n",
    "# 2) Map labels to integers for the model\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# 3) Text tokenizer (you can change the model name later)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 4) Image transforms (simple version, you can tweak later)\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),                  # [0,1]\n",
    "    transforms.Normalize(                   # ImageNet-style normalization\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "class MVSADataset(Dataset):\n",
    "    def __init__(self, labels_df, data_dir, tokenizer, image_transform=None, max_length=64):\n",
    "        self.labels_df = labels_df.reset_index(drop=True)\n",
    "        self.data_dir = data_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = image_transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        sample_id = str(row[\"ID\"])\n",
    "        label_str = row[\"final_label\"]\n",
    "        label = label2id[label_str]\n",
    "\n",
    "        # --- Load text only ---\n",
    "        txt_path = os.path.join(self.data_dir, f\"{sample_id}.txt\")\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read().strip()\n",
    "\n",
    "        # Tokenize text\n",
    "        text_enc = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Squeeze tokenizer batch dimension\n",
    "        item = {\n",
    "            \"input_ids\": text_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": text_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "            \"id\": sample_id,\n",
    "        }\n",
    "        return item\n",
    "\n",
    "# 5) Create one dataset instance (you'll later split into train/val/test)\n",
    "dataset = MVSADataset(\n",
    "    labels_df=labels_df,\n",
    "    data_dir=DATA_DIR,\n",
    "    tokenizer=tokenizer,\n",
    "    image_transform=image_transform,\n",
    "    max_length=64,\n",
    ")\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "sample = dataset[0]\n",
    "print(\"Keys in one sample:\", sample.keys())\n",
    "print(\"input_ids shape:\", sample[\"input_ids\"].shape)\n",
    "print(\"label:\", sample[\"label\"], \"→\", id2label[sample[\"label\"].item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "86798e47-48ad-4a02-a9d9-796fc856fb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 13720\n",
      "Val size:   2940\n",
      "Test size:  2940\n",
      "Train batches: 858\n",
      "Val batches:   184\n",
      "Test batches:  184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1) Train/Val/Test split (70/15/15), stratified by label\n",
    "train_df, temp_df = train_test_split(\n",
    "    labels_df,\n",
    "    test_size=0.30,\n",
    "    stratify=labels_df[\"final_label\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.50,\n",
    "    stratify=temp_df[\"final_label\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:  \", len(val_df))\n",
    "print(\"Test size: \", len(test_df))\n",
    "\n",
    "# 2) Create Dataset objects for each split\n",
    "train_dataset = MVSADataset(\n",
    "    labels_df=train_df,\n",
    "    data_dir=DATA_DIR,\n",
    "    tokenizer=tokenizer,\n",
    "    image_transform=image_transform,\n",
    "    max_length=64,\n",
    ")\n",
    "\n",
    "val_dataset = MVSADataset(\n",
    "    labels_df=val_df,\n",
    "    data_dir=DATA_DIR,\n",
    "    tokenizer=tokenizer,\n",
    "    image_transform=image_transform,\n",
    "    max_length=64,\n",
    ")\n",
    "\n",
    "test_dataset = MVSADataset(\n",
    "    labels_df=test_df,\n",
    "    data_dir=DATA_DIR,\n",
    "    tokenizer=tokenizer,\n",
    "    image_transform=image_transform,\n",
    "    max_length=64,\n",
    ")\n",
    "\n",
    "# 3) Create DataLoaders\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Val batches:  \", len(val_loader))\n",
    "print(\"Test batches: \", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61b846d0-b4b3-4642-8ad2-de651fef2db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1/3 | train_loss=0.8189 | val_loss=0.7473 | val_acc=0.6303\n",
      "Epoch 2/3 | train_loss=0.6994 | val_loss=0.7491 | val_acc=0.6255\n",
      "Epoch 3/3 | train_loss=0.5774 | val_loss=0.8183 | val_acc=0.6143\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "\n",
    "# 1) Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 2) Text-only baseline model (BERT)\n",
    "num_labels = 3  # negative, neutral, positive\n",
    "text_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "text_model.to(device)\n",
    "\n",
    "# 3) Optimizer, scheduler, loss\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "\n",
    "optimizer = AdamW(text_model.parameters(), lr=LR)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 4) Simple training + validation loop (text-only baseline)\n",
    "def train_one_epoch(model, data_loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def eval_one_epoch(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(text_model, train_loader)\n",
    "    val_loss, val_acc = eval_one_epoch(text_model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d18dcde9-335c-4571-af8f-f9d32ff9854a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6105\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.3750    0.3018    0.3344       169\n",
      "     neutral     0.6154    0.5681    0.5908      1366\n",
      "    positive     0.6273    0.6890    0.6567      1405\n",
      "\n",
      "    accuracy                         0.6105      2940\n",
      "   macro avg     0.5392    0.5196    0.5273      2940\n",
      "weighted avg     0.6073    0.6105    0.6076      2940\n",
      "\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[ 51  82  36]\n",
      " [ 51 776 539]\n",
      " [ 34 403 968]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Make sure we're in eval mode\n",
    "text_model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "# Overall accuracy\n",
    "correct = sum(int(p == y) for p, y in zip(all_preds, all_labels))\n",
    "accuracy = correct / len(all_labels)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# If you still have label2id / id2label from before:\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "target_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
    "\n",
    "print(\"\\nConfusion matrix (rows=true, cols=pred):\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6134f151-1079-4c5c-aae0-25265cca48da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image-only train batches: 429\n",
      "Image-only val batches:   92\n",
      "Image-only test batches:  92\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, UnidentifiedImageError, ImageFile\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Allow loading truncated images (PIL will be less strict)\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Image transform (same as before)\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),                  # [0, 1]\n",
    "    transforms.Normalize(                   # standard ImageNet normalization\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Label mapping (same as before)\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "\n",
    "class MVSAImageDataset(Dataset):\n",
    "    def __init__(self, labels_df, data_dir, image_transform=None):\n",
    "        self.labels_df = labels_df.reset_index(drop=True)\n",
    "        self.data_dir = data_dir\n",
    "        self.image_transform = image_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        sample_id = str(row[\"ID\"])\n",
    "        label_str = row[\"final_label\"]\n",
    "        label = label2id[label_str]\n",
    "\n",
    "        img_path = os.path.join(self.data_dir, f\"{sample_id}.jpg\")\n",
    "\n",
    "        # Try to load the image; if corrupted/missing/truncated, use a dummy black image\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            image = image.convert(\"RGB\")\n",
    "        except (FileNotFoundError, UnidentifiedImageError, OSError):\n",
    "            # 224x224 black image as fallback\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "\n",
    "        if self.image_transform is not None:\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "            \"id\": sample_id,\n",
    "        }\n",
    "\n",
    "\n",
    "# Create image-only datasets using the same splits as text\n",
    "img_train_dataset = MVSAImageDataset(train_df, DATA_DIR, image_transform=image_transform)\n",
    "img_val_dataset   = MVSAImageDataset(val_df,   DATA_DIR, image_transform=image_transform)\n",
    "img_test_dataset  = MVSAImageDataset(test_df,  DATA_DIR, image_transform=image_transform)\n",
    "\n",
    "# Image dataloaders (keep num_workers=0 on Mac to avoid multiprocessing issues)\n",
    "IMG_BATCH_SIZE = 32\n",
    "\n",
    "img_train_loader = DataLoader(img_train_dataset, batch_size=IMG_BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "img_val_loader   = DataLoader(img_val_dataset,   batch_size=IMG_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "img_test_loader  = DataLoader(img_test_dataset,  batch_size=IMG_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Image-only train batches:\", len(img_train_loader))\n",
    "print(\"Image-only val batches:  \", len(img_val_loader))\n",
    "print(\"Image-only test batches: \", len(img_test_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f745f0e5-c9a4-4606-99ab-b37efdc6dea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for image model: cpu\n",
      "\n",
      "[Image-only] Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sahilanjum/Anaconda/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sahilanjum/Anaconda/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [img train] step 100/429  loss=0.9671\n",
      "  [img train] step 200/429  loss=0.8587\n",
      "  [img train] step 300/429  loss=0.9306\n",
      "  [img train] step 400/429  loss=0.9194\n",
      "Epoch 1/3 | train_loss=0.9350 | val_loss=0.8453 | val_acc=0.5548\n",
      "\n",
      "[Image-only] Epoch 2/3\n",
      "  [img train] step 100/429  loss=0.6460\n",
      "  [img train] step 200/429  loss=0.8919\n",
      "  [img train] step 300/429  loss=0.5579\n",
      "  [img train] step 400/429  loss=0.6164\n",
      "Epoch 2/3 | train_loss=0.7063 | val_loss=0.8799 | val_acc=0.5466\n",
      "\n",
      "[Image-only] Epoch 3/3\n",
      "  [img train] step 100/429  loss=0.4342\n",
      "  [img train] step 200/429  loss=0.3832\n",
      "  [img train] step 300/429  loss=0.4459\n",
      "  [img train] step 400/429  loss=0.3990\n",
      "Epoch 3/3 | train_loss=0.4469 | val_loss=0.9303 | val_acc=0.5476\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torchvision import models\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# 1) Device (reuse if already defined, otherwise this is safe)\n",
    "if \"device\" not in globals():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "print(\"Using device for image model:\", device)\n",
    "\n",
    "# 2) Image-only baseline model (ResNet18)\n",
    "num_labels = 3  # negative, neutral, positive\n",
    "\n",
    "img_model = models.resnet18(pretrained=True)\n",
    "in_features = img_model.fc.in_features\n",
    "img_model.fc = nn.Linear(in_features, num_labels)\n",
    "img_model = img_model.to(device)\n",
    "\n",
    "# 3) Optimizer, scheduler, loss\n",
    "EPOCHS_IMG = 3\n",
    "LR_IMG = 1e-4\n",
    "\n",
    "img_optimizer = AdamW(img_model.parameters(), lr=LR_IMG)\n",
    "img_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "total_steps_img = len(img_train_loader) * EPOCHS_IMG\n",
    "img_scheduler = get_linear_schedule_with_warmup(\n",
    "    img_optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps_img),\n",
    "    num_training_steps=total_steps_img,\n",
    ")\n",
    "\n",
    "# 4) Training + validation loops for image-only model\n",
    "\n",
    "def train_one_epoch_img(model, data_loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        img_optimizer.zero_grad()\n",
    "\n",
    "        images = batch[\"image\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = img_criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        img_optimizer.step()\n",
    "        img_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % 100 == 0:\n",
    "            print(f\"  [img train] step {step+1}/{len(data_loader)}  loss={loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def eval_one_epoch_img(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "            loss = img_criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS_IMG):\n",
    "    print(f\"\\n[Image-only] Epoch {epoch+1}/{EPOCHS_IMG}\")\n",
    "    train_loss_img = train_one_epoch_img(img_model, img_train_loader)\n",
    "    val_loss_img, val_acc_img = eval_one_epoch_img(img_model, img_val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS_IMG} | train_loss={train_loss_img:.4f} | val_loss={val_loss_img:.4f} | val_acc={val_acc_img:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c48fad3-022f-43f9-9a84-fa3ae7725469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Image-only] Test accuracy: 0.5527\n",
      "\n",
      "[Image-only] Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.3051    0.1065    0.1579       169\n",
      "     neutral     0.5398    0.5703    0.5546      1366\n",
      "    positive     0.5758    0.5893    0.5825      1405\n",
      "\n",
      "    accuracy                         0.5527      2940\n",
      "   macro avg     0.4736    0.4220    0.4317      2940\n",
      "weighted avg     0.5435    0.5527    0.5451      2940\n",
      "\n",
      "\n",
      "[Image-only] Confusion matrix (rows=true, cols=pred):\n",
      "[[ 18  99  52]\n",
      " [ 29 779 558]\n",
      " [ 12 565 828]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Ensure eval mode\n",
    "img_model.eval()\n",
    "\n",
    "all_img_preds = []\n",
    "all_img_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in img_test_loader:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        logits = img_model(images)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        all_img_preds.extend(preds.cpu().tolist())\n",
    "        all_img_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "# Overall accuracy\n",
    "correct_img = sum(int(p == y) for p, y in zip(all_img_preds, all_img_labels))\n",
    "accuracy_img = correct_img / len(all_img_labels)\n",
    "print(f\"[Image-only] Test accuracy: {accuracy_img:.4f}\")\n",
    "\n",
    "# Label names (reuse mapping if already defined)\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "target_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "print(\"\\n[Image-only] Classification report:\")\n",
    "print(classification_report(all_img_labels, all_img_preds, target_names=target_names, digits=4))\n",
    "\n",
    "print(\"\\n[Image-only] Confusion matrix (rows=true, cols=pred):\")\n",
    "print(confusion_matrix(all_img_labels, all_img_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00fc88d5-1891-4555-9e88-3a650a88078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal train batches: 858\n",
      "Multimodal val batches:   184\n",
      "Multimodal test batches:  184\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, UnidentifiedImageError, ImageFile\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Make sure truncated images don't crash us\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Reuse label2id, tokenizer, image_transform, DATA_DIR, train_df/val_df/test_df\n",
    "\n",
    "class MVSAMultimodalDataset(Dataset):\n",
    "    def __init__(self, labels_df, data_dir, tokenizer, image_transform=None, max_length=64):\n",
    "        self.labels_df = labels_df.reset_index(drop=True)\n",
    "        self.data_dir = data_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = image_transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        sample_id = str(row[\"ID\"])\n",
    "        label_str = row[\"final_label\"]\n",
    "        label = label2id[label_str]\n",
    "\n",
    "        # --- Load text ---\n",
    "        txt_path = os.path.join(self.data_dir, f\"{sample_id}.txt\")\n",
    "        try:\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                text = f.read().strip()\n",
    "        except FileNotFoundError:\n",
    "            text = \"\"  # fallback: empty text if missing\n",
    "\n",
    "        text_enc = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # --- Load image ---\n",
    "        img_path = os.path.join(self.data_dir, f\"{sample_id}.jpg\")\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            image = image.convert(\"RGB\")\n",
    "        except (FileNotFoundError, UnidentifiedImageError, OSError):\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))  # dummy black\n",
    "\n",
    "        if self.image_transform is not None:\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": text_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": text_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"image\": image,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "            \"id\": sample_id,\n",
    "        }\n",
    "\n",
    "\n",
    "# Create multimodal datasets\n",
    "mm_train_dataset = MVSAMultimodalDataset(\n",
    "    train_df, DATA_DIR, tokenizer, image_transform=image_transform, max_length=64\n",
    ")\n",
    "mm_val_dataset = MVSAMultimodalDataset(\n",
    "    val_df, DATA_DIR, tokenizer, image_transform=image_transform, max_length=64\n",
    ")\n",
    "mm_test_dataset = MVSAMultimodalDataset(\n",
    "    test_df, DATA_DIR, tokenizer, image_transform=image_transform, max_length=64\n",
    ")\n",
    "\n",
    "# Multimodal loaders (batch smaller because text+image together are heavier)\n",
    "MM_BATCH_SIZE = 16\n",
    "\n",
    "mm_train_loader = DataLoader(mm_train_dataset, batch_size=MM_BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "mm_val_loader   = DataLoader(mm_val_dataset,   batch_size=MM_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "mm_test_loader  = DataLoader(mm_test_dataset,  batch_size=MM_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Multimodal train batches:\", len(mm_train_loader))\n",
    "print(\"Multimodal val batches:  \", len(mm_val_loader))\n",
    "print(\"Multimodal test batches: \", len(mm_test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f021f1ba-37a3-4558-8ed1-c61f189f1978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for multimodal model: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sahilanjum/Anaconda/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sahilanjum/Anaconda/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label counts:\n",
      " final_label\n",
      "positive    6557\n",
      "neutral     6376\n",
      "negative     787\n",
      "Name: count, dtype: int64\n",
      "Class weights (neg, neu, pos): [5.811097145080566, 0.7172731161117554, 0.6974734663963318]\n",
      "\n",
      "[Multimodal] Epoch 1/3\n",
      "  [mm train] step 100/858  loss=1.1394\n",
      "  [mm train] step 200/858  loss=0.9574\n",
      "  [mm train] step 300/858  loss=0.8651\n",
      "  [mm train] step 400/858  loss=0.7194\n",
      "  [mm train] step 500/858  loss=1.0653\n",
      "  [mm train] step 600/858  loss=0.7991\n",
      "  [mm train] step 700/858  loss=0.9953\n",
      "  [mm train] step 800/858  loss=1.0416\n",
      "Epoch 1/3 | train_loss=0.9382 | val_loss=0.8564 | val_acc=0.5401\n",
      "\n",
      "[Multimodal] Epoch 2/3\n",
      "  [mm train] step 100/858  loss=0.4485\n",
      "  [mm train] step 200/858  loss=0.9615\n",
      "  [mm train] step 300/858  loss=0.3626\n",
      "  [mm train] step 400/858  loss=0.6684\n",
      "  [mm train] step 500/858  loss=0.5358\n",
      "  [mm train] step 600/858  loss=1.6227\n",
      "  [mm train] step 700/858  loss=0.5253\n",
      "  [mm train] step 800/858  loss=0.4777\n",
      "Epoch 2/3 | train_loss=0.7322 | val_loss=0.8557 | val_acc=0.5915\n",
      "\n",
      "[Multimodal] Epoch 3/3\n",
      "  [mm train] step 100/858  loss=1.1158\n",
      "  [mm train] step 200/858  loss=0.3782\n",
      "  [mm train] step 300/858  loss=0.4569\n",
      "  [mm train] step 400/858  loss=0.4086\n",
      "  [mm train] step 500/858  loss=0.5890\n",
      "  [mm train] step 600/858  loss=0.4933\n",
      "  [mm train] step 700/858  loss=0.6054\n",
      "  [mm train] step 800/858  loss=0.5540\n",
      "Epoch 3/3 | train_loss=0.5674 | val_loss=0.9325 | val_acc=0.6007\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torchvision import models\n",
    "from transformers import AutoModel, get_linear_schedule_with_warmup\n",
    "\n",
    "# 1) Device (reuse if already defined)\n",
    "if \"device\" not in globals():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "print(\"Using device for multimodal model:\", device)\n",
    "\n",
    "num_labels = 3  # negative, neutral, positive\n",
    "\n",
    "# 2) Multimodal model: BERT (text) + ResNet18 (image) + fusion head\n",
    "class MultimodalSentimentModel(nn.Module):\n",
    "    def __init__(self, text_model_name=\"bert-base-uncased\", num_labels=3):\n",
    "        super().__init__()\n",
    "        # Text encoder (CLS embedding)\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        text_hidden = self.text_encoder.config.hidden_size\n",
    "\n",
    "        # Image encoder (ResNet18 without final FC)\n",
    "        self.img_encoder = models.resnet18(pretrained=True)\n",
    "        img_hidden = self.img_encoder.fc.in_features\n",
    "        self.img_encoder.fc = nn.Identity()\n",
    "\n",
    "        # Fusion + classifier\n",
    "        fusion_dim = text_hidden + img_hidden\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # Text: CLS token embedding\n",
    "        text_out = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        # CLS token is position 0\n",
    "        text_cls = text_out.last_hidden_state[:, 0, :]  # (batch, hidden)\n",
    "\n",
    "        # Image features\n",
    "        img_feat = self.img_encoder(image)  # (batch, img_hidden)\n",
    "\n",
    "        # Fuse\n",
    "        fused = torch.cat([text_cls, img_feat], dim=1)  # (batch, fusion_dim)\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "mm_model = MultimodalSentimentModel(num_labels=num_labels).to(device)\n",
    "\n",
    "# 3) Class weights from train_df (to handle imbalance)\n",
    "class_counts = train_df[\"final_label\"].value_counts()\n",
    "print(\"Train label counts:\\n\", class_counts)\n",
    "\n",
    "labels_order = [\"negative\", \"neutral\", \"positive\"]\n",
    "total = len(train_df)\n",
    "weights_list = []\n",
    "for lbl in labels_order:\n",
    "    # simple inverse-frequency style: higher weight for rarer classes\n",
    "    count = class_counts[lbl]\n",
    "    w = total / (num_labels * count)\n",
    "    weights_list.append(w)\n",
    "\n",
    "class_weights = torch.tensor(weights_list, dtype=torch.float32).to(device)\n",
    "print(\"Class weights (neg, neu, pos):\", class_weights.tolist())\n",
    "\n",
    "mm_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# 4) Optimizer & scheduler\n",
    "EPOCHS_MM = 3\n",
    "LR_MM = 2e-5  # you can tune this later\n",
    "\n",
    "mm_optimizer = AdamW(mm_model.parameters(), lr=LR_MM)\n",
    "\n",
    "total_steps_mm = len(mm_train_loader) * EPOCHS_MM\n",
    "mm_scheduler = get_linear_schedule_with_warmup(\n",
    "    mm_optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps_mm),\n",
    "    num_training_steps=total_steps_mm,\n",
    ")\n",
    "\n",
    "# 5) Training + validation loops\n",
    "\n",
    "def train_one_epoch_mm(model, data_loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        mm_optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        images = batch[\"image\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            image=images,\n",
    "        )\n",
    "        loss = mm_criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        mm_optimizer.step()\n",
    "        mm_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % 100 == 0:\n",
    "            print(f\"  [mm train] step {step+1}/{len(data_loader)}  loss={loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def eval_one_epoch_mm(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            images = batch[\"image\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                image=images,\n",
    "            )\n",
    "            loss = mm_criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS_MM):\n",
    "    print(f\"\\n[Multimodal] Epoch {epoch+1}/{EPOCHS_MM}\")\n",
    "    train_loss_mm = train_one_epoch_mm(mm_model, mm_train_loader)\n",
    "    val_loss_mm, val_acc_mm = eval_one_epoch_mm(mm_model, mm_val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS_MM} | train_loss={train_loss_mm:.4f} | val_loss={val_loss_mm:.4f} | val_acc={val_acc_mm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2929327e-83e2-44c4-b905-25362d1bd9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Multimodal] Test accuracy: 0.6126\n",
      "\n",
      "[Multimodal] Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.3038    0.5266    0.3853       169\n",
      "     neutral     0.6262    0.5813    0.6029      1366\n",
      "    positive     0.6657    0.6534    0.6595      1405\n",
      "\n",
      "    accuracy                         0.6126      2940\n",
      "   macro avg     0.5319    0.5871    0.5492      2940\n",
      "weighted avg     0.6265    0.6126    0.6174      2940\n",
      "\n",
      "\n",
      "[Multimodal] Confusion matrix (rows=true, cols=pred):\n",
      "[[ 89  49  31]\n",
      " [142 794 430]\n",
      " [ 62 425 918]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Make sure model is in eval mode\n",
    "mm_model.eval()\n",
    "\n",
    "all_mm_preds = []\n",
    "all_mm_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in mm_test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        images = batch[\"image\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        logits = mm_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            image=images,\n",
    "        )\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        all_mm_preds.extend(preds.cpu().tolist())\n",
    "        all_mm_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "# Overall accuracy\n",
    "correct_mm = sum(int(p == y) for p, y in zip(all_mm_preds, all_mm_labels))\n",
    "accuracy_mm = correct_mm / len(all_mm_labels)\n",
    "print(f\"[Multimodal] Test accuracy: {accuracy_mm:.4f}\")\n",
    "\n",
    "# Label names\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "target_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "print(\"\\n[Multimodal] Classification report:\")\n",
    "print(classification_report(all_mm_labels, all_mm_preds, target_names=target_names, digits=4))\n",
    "\n",
    "print(\"\\n[Multimodal] Confusion matrix (rows=true, cols=pred):\")\n",
    "print(confusion_matrix(all_mm_labels, all_mm_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62197a07-18f7-4c8d-9a03-bde730498d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
