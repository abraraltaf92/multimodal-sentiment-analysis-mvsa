# Multimodal Sentiment Analysis on the MVSA Dataset

---

## ğŸ“˜ Project Overview
This project performs **sentiment analysis** on social media posts using **multimodal learning** â€” combining both **images and text**.  
The dataset used is **MVSA-Multiple**, which provides tweets paired with images and annotated sentiment labels.

We implement and compare three models:
1. **Text-Only Model (BERT)**
2. **Image-Only Model (ResNet18)**
3. **Multimodal Fusion Model (BERT + ResNet18)**

---

## ğŸ¯ Problem Statement
Single-modality sentiment models (text-only or image-only) often fail to capture the full meaning of a social media post.  
Many posts rely on **sarcasm, memes, visual emotion cues**, or **ambiguous text**, making unimodal learning unreliable.

**Goal:** Build a multimodal deep-learning system that integrates both textual context and visual emotion cues to improve sentiment classification performance across:
- Positive  
- Neutral  
- Negative  

---

## ğŸ“‚ Dataset: MVSA-Multiple
- **Samples:** 19,600 social media posts  
- Each sample contains:
  - An image
  - A text caption
  - Labels from 3 annotators (text + image sentiments)

### âœ” Label Cleaning
You apply **majority voting** to produce a final unified sentiment label.

Label distribution:
- **Positive:** 9367  
- **Neutral:** 9108  
- **Negative:** 1125  

### ğŸ”— Dataset Link  
[https://mmajlab.org/dataset/mvsa/](https://mcrlab.net/research/mvsa-sentiment-analysis-on-multi-view-social-data/)

---

## ğŸ§  Model Architectures

### 1ï¸âƒ£ Text-Only Model (BERT)
```
[Text] â†’ [Tokenizer] â†’ [BERT Encoder] â†’ [CLS] â†’ [Classifier] â†’ Sentiment
```

### 2ï¸âƒ£ Image-Only Model (ResNet18)
```
[Image] â†’ [ResNet18 Backbone] â†’ [Feature Vector] â†’ [Classifier] â†’ Sentiment
```

### 3ï¸âƒ£ Multimodal Fusion Model (BERT + ResNet18)
```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€ BERT Embedding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
[Text Input] â”€â”€â”€â”¤                                  â”œâ”€â”€â–º Concat â”€â–º Dense â”€â–º Output
                â””â”€â”€â”€â”€â”€â”€â”€â”€ ResNet18 Embedding â”€â”€â”€â”€â”€â”€â”˜
[Image Input] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Results Summary

| Model | Test Accuracy |
|-------|--------------|
| **Text-Only (BERT)** | **95.05%** |
| **Image-Only (ResNet18)** | **91.27%** |
| **Multimodal (BERT + ResNet18)** | **92.26%** |

---

## ğŸš€ How to Run

### 1. Clone repo
```bash
git clone https://github.com/<your-username>/multimodal-sentiment-analysis-mvsa
cd multimodal-sentiment-analysis-mvsa
```

### 2. Prepare dataset structure
```
/data/
    2499.jpg
    2499.txt
labels_clean.csv
```

### 3. Train models
```bash
# Train text-only model
python train_text.py

# Train image-only model
python train_image.py

# Train multimodal fusion model
python train_multimodal.py
```

---

## ğŸ“ Citation
If you use the MVSA dataset, please cite:

```
@inproceedings{MVSA,
  author    = {Teng Niu and Shiai Zhu and Lei Pang and Abdulmotaleb El{-}Saddik},
  title     = {Sentiment Analysis on Multi-View Social Data},
  booktitle = {MultiMedia Modeling},
  pages     = {15--27},
  year      = {2016},
}
```

---
